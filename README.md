# Agentic AI Framework  
### Event-Driven AI Orchestration using Airflow & Kafka

This project demonstrates how an **agent-based AI workflow** can be built using
Apache Airflow for orchestration and Apache Kafka for event-driven communication.

The goal of the project is to show how tasks can be routed asynchronously through
an **Agent Router**, instead of being tightly coupled inside a single workflow.

---

## ğŸ” What this project does

- Airflow triggers a workflow
- A message is published to Kafka
- An Agent Router listens to Kafka
- The agent processes the message
- The result is published back to Kafka
- Airflow consumes the processed output

This pattern is useful for building **scalable, decoupled AI systems**.

---

## ğŸ§± Architecture Flow

Airflow DAG
â†“
Kafka (ai_agent_input)
â†“
Agent Router
â†“
Kafka (ai_agent_output)
â†“
Airflow Consumer DAG

Each component runs independently using Docker.

---

## ğŸ§© Components Explained

### Apache Airflow
- Manages and triggers workflows
- Sends messages to Kafka
- Can react to processed results

### Apache Kafka
- Acts as the communication backbone
- Enables asynchronous processing
- Topics used:
  - `ai_agent_input`
  - `ai_agent_output`

### Agent Router
- A Python service running in its own container
- Uses KafkaConsumer and KafkaProducer
- Listens, processes, and forwards messages

---

## ğŸ› ï¸ Tech Stack

- Python 3.12  
- Apache Airflow 2.9.3  
- Apache Kafka (Confluent)  
- PostgreSQL  
- Redis  
- Docker & Docker Compose  

---

â–¶ï¸ How to Run the Project
ğŸ”§ Requirements

Make sure the following are installed on your system:

Docker

Docker Compose

â–¶ï¸ Start All Services

From the project root directory, run:

docker compose up -d


This will start the following services:

PostgreSQL

Redis

Zookeeper

Kafka

Airflow (Webserver + Scheduler)

Agent Router

ğŸ” Check Running Containers

To verify that all services are running correctly:

docker compose ps


All containers should show Up status.

â–¶ï¸ Trigger the Workflow
1ï¸âƒ£ Unpause the Producer DAG
docker exec -it airflow-webserver airflow dags unpause kafka_producer_dag

2ï¸âƒ£ Trigger the DAG Manually
docker exec -it airflow-webserver airflow dags trigger kafka_producer_dag


This sends a message from Airflow to Kafka.

ğŸ“¤ Expected Output

When everything is working correctly, check the Agent Router logs:

docker logs -f agent-router


You should see output similar to this:

Agentic Router starting...
Connected to Kafka at kafka:9092
Listening on topic: ai_agent_input
Received message: {"type":"test","content":"hello from terminal"}
Sent processed message to ai_agent_output

âœ… What This Confirms

Airflow successfully produced a Kafka message

The Agent Router consumed the message

The message was processed and forwarded

ğŸ” Final Result

The pipeline works end-to-end:

Airflow â†’ Kafka â†’ Agent Router â†’ Kafka â†’ Airflow

ğŸ“ Project Structure
ai_agent_framework/
â”‚
â”œâ”€â”€ dags/                 # Airflow DAG definitions
â”œâ”€â”€ src/agents/           # Agent logic
â”œâ”€â”€ message_router.py     # Kafka agent router
â”œâ”€â”€ Dockerfile            # Agent Router image
â”œâ”€â”€ Dockerfile.airflow    # Airflow image
â”œâ”€â”€ docker-compose.yml    # Service orchestration
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ§  What I Learned From This Project

How event-driven systems work

Integrating Airflow with Kafka

Designing loosely-coupled AI pipelines

Containerizing distributed systems

Debugging real-world Docker & Kafka issues

ğŸš€ Possible Improvements

Add multiple agent types

Implement retry and failure handling

Persist agent state

Add monitoring and metrics
ğŸ‘¤ Author

Bhavik Raman
Agentic AI â€¢ Distributed Systems â€¢ Data Engineering


